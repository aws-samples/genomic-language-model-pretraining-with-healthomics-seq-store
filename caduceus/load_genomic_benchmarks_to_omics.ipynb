{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51ff32d0-7279-4c64-8f70-7ca02b28f9d1",
   "metadata": {},
   "source": [
    "## Loading Genomic Benchmarks into AWS HealthOmics (Optional)\n",
    "\n",
    "[AWS HealthOmics](https://aws.amazon.com/healthomics/) is a purpose-built service that helps healthcare and life science organizations and their software partners store, query, and analyze genomic, transcriptomic, and other omics data and then generate insights from that data to improve health. It supports large-scale analysis and collaborative research.\n",
    "\n",
    "In this scenario, we will be fine-tuning a pre-trained [Caduceus](https://caduceus-dna.github.io/) model for a range of DNA sequence classification tasks published on [Genomic Benchmarks](https://bmcgenomdata.biomedcentral.com/articles/10.1186/s12863-023-01123-8) (Grešová, K., Martinek, V., Čechák, D. et al., 2023).\n",
    "\n",
    "The Genomic Benchmark datasets are publicly available on [HuggingFace](https://huggingface.co/katarinagresova), but you can optionally use this notebook to convert the datasets into FASTQ format and import them into an AWS HealthOmics Sequence Store. In the training phase, you can choose to read the datasets from this Sequence Store or download directly from HuggingFace. \n",
    "\n",
    "So while this use case leverages publicly available data, the HealthOmics integration demonstrates an alternative workflow that may be useful for genomics research institutions who wish to train models on their own proprietary DNA sequence datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d20e8-91c0-4df4-8c0d-32ed21b34ea0",
   "metadata": {},
   "source": [
    "## 0. Prerequisites\n",
    "\n",
    "First, create a bucket that you can use for \"staging\" the FASTQ files before they are imported into HealthOmics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c51f490-6c47-44f4-a601-21916de1a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "account_id = sagemaker.Session().account_id()\n",
    "\n",
    "S3_BUCKET = f\"genomic-benchmarks-staging-{account_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73a94ef7-0884-4795-b5ce-cae18d3b55b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Location\": \"/genomic-benchmarks-staging-767398100082\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws s3api create-bucket --bucket \"$S3_BUCKET\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02b46a4-c608-4780-a4a2-838904564b70",
   "metadata": {},
   "source": [
    "Next, create a service role for `omics.amazonaws.com` to be able to access the staged datasets and import the readsets. More details in the [User Guide](https://docs.aws.amazon.com/omics/latest/dev/create-reference-store.html#api-create-reference-store).\n",
    "\n",
    "Use the following trust policy:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"omics.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "And make sure it has permissions to objects from the bucket you just created, e.g.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:GetBucketLocation\"\n",
    "                \n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:s3:::<REPLACE_ME_WITH_S3_BUCKET>\",\n",
    "                \"arn:aws:s3:::<REPLACE_ME_WITH_S3_BUCKET>/*\"\n",
    "            ]\n",
    "         }\n",
    "      ]\n",
    "   }   \n",
    "}\n",
    "```\n",
    "\n",
    "For this notebook, we will assume the role is named `OmicsImportRole`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b66c1f4-da3c-436a-aebc-a15f018d317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assumes you named the role `OmicsImportRole`\n",
    "IMPORT_JOB_ROLE_ARN = f\"arn:aws:iam::{account_id}:role/OmicsImportRole\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebecaccc-3401-4e7f-9dba-001a79c3b939",
   "metadata": {},
   "source": [
    "## 1. Create the HealthOmics Sequence Store\n",
    "\n",
    "HealthOmics sequence stores allow you to store genomic files in common formats like FASTQ and BAM. In this case, we will be converting the Genomic Benchmark sequences into gzip-compressed FASTQ files.\n",
    "\n",
    "Before we can import the genomic files (also called read sets), we need to create the sequence store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c61bca3-3132-401a-8281-dd9fb338a390",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_STORE_NAME = \"genomic_benchmarks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68207231-ab69-4b97-bc4b-1d6230d6a20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence store ID: 9757315158\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from datasets import load_dataset\n",
    "from scripts.omics_utils import create_fastq_entry, gzip_fileobj\n",
    "\n",
    "omics = boto3.client(\"omics\")\n",
    "\n",
    "seq_store_resp = omics.create_sequence_store(\n",
    "    name=SEQUENCE_STORE_NAME,\n",
    "    description=\"Genomic Benchmarks datasets for DNA sequence classification | https://bmcgenomdata.biomedcentral.com/articles/10.1186/s12863-023-01123-8#citeas\",\n",
    ")\n",
    "seq_store_id = seq_store_resp[\"id\"]\n",
    "\n",
    "# take note of this sequence store ID for the next notebook\n",
    "print(f\"Sequence store ID: {seq_store_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6f5752-fb91-42f2-9b02-b7bec3e298af",
   "metadata": {},
   "source": [
    "Take note of the Sequence Store ID, as you'll need it in the next notebook (training the model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e28ab6-07e3-4a1e-a9df-33496b0274fb",
   "metadata": {},
   "source": [
    "## 2. Download the Datasets, Convert to FASTQ, and Upload to S3\n",
    "\n",
    "The following loop will download each benchmark task's dataset from  HuggingFace, convert it into gzipped FASTQ format (with dummy values for the quality scores, since they are not used in the classification task), and upload it to S3. There will be 1 file per task per split (train/test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403742c-0f4f-42bf-9393-d51a6b140194",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "# loop through all the tasks\n",
    "# https://huggingface.co/katarinagresova/Genomic_Benchmarks_{task}\n",
    "tasks = [\n",
    "    \"demo_coding_vs_intergenomic_seqs\",\n",
    "    \"demo_human_or_worm\",\n",
    "    \"dummy_mouse_enhancers_ensembl\",\n",
    "    \"human_enhancers_cohn\",\n",
    "    \"human_enhancers_ensembl\",\n",
    "    \"human_ensembl_regulatory\",\n",
    "    \"human_nontata_promoters\",\n",
    "    \"human_ocr_ensembl\",\n",
    "]\n",
    "\n",
    "sources = []\n",
    "for task in tasks:\n",
    "    print(f\"Preparing task: {task}\")\n",
    "    s3_prefix = f\"genomic_benchmarks/{task}/\"\n",
    "    s3_uris = []\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        # download the HF dataset and convert to FASTQ\n",
    "        ds = load_dataset(f\"katarinagresova/Genomic_Benchmarks_{task}\", split=split)\n",
    "    \n",
    "        fastq_content = \"\"\n",
    "        for idx, row in enumerate(ds):\n",
    "            fastq_content += create_fastq_entry(\n",
    "                sequence=row[\"seq\"],\n",
    "                sequence_id=f\"label_{row['label']}_idx_{idx}\",\n",
    "            )\n",
    "        # upload FASTQ's to S3\n",
    "        s3_key = os.path.join(s3_prefix, split, f\"{split}_combined.fastq.gz\")\n",
    "        s3_client.put_object(\n",
    "            Bucket=S3_BUCKET,\n",
    "            Key=s3_key,\n",
    "            Body=gzip_fileobj(io.StringIO(fastq_content)),\n",
    "        )\n",
    "        s3_uris.append(f\"s3://{S3_BUCKET}/{s3_key}\")\n",
    "        print(f\"Successfully uploaded {s3_key} with {len(ds)} sequences\")\n",
    "\n",
    "    # include the task name in the metadata for the read set\n",
    "    # useful for filtering when reading data\n",
    "    sources += [\n",
    "        {\n",
    "            \"sourceFiles\": {\"source1\": s3_uri},\n",
    "            \"sourceFileType\": \"FASTQ\",\n",
    "            \"subjectId\": task,\n",
    "            \"sampleId\": task,\n",
    "        }\n",
    "        for s3_uri in s3_uris\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a90d41-e59f-4564-8609-270bba136a61",
   "metadata": {},
   "source": [
    "## 3. Create a Read Set Import Job\n",
    "\n",
    "Finally, we can create a Read Set Import job to take the FASTQ files from our staging bucket and add them to the sequence store. Note that we included the task ID as the `subjectId` (`sources` in the above loop) - this will help us retrieve the read sets for each task later, when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e700338f-e63e-4f18-835e-5c9e913bb607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a readset from these FASTQ files\n",
    "import_job_resp = omics.start_read_set_import_job(\n",
    "    sequenceStoreId=seq_store_id,\n",
    "    roleArn=IMPORT_JOB_ROLE_ARN,\n",
    "    sources=sources,\n",
    ")\n",
    "import_job_id = import_job_resp[\"id\"]\n",
    "print(f\"Import job ID: {import_job_id}\")\n",
    "\n",
    "waiter = omics.get_waiter('read_set_import_job_completed')\n",
    "waiter.wait(\n",
    "    id=import_job_id,\n",
    "    sequenceStoreId=seq_store_id,\n",
    ")\n",
    "\n",
    "print(\"Job complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
