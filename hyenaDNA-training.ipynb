{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0dfbb7c-2280-4b01-949a-91238a892d0b",
   "metadata": {},
   "source": [
    "### 0. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82308943-5300-4cc0-856c-879e1a6f24fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU pip\n",
    "%pip install -qU sagemaker boto3 awscli boto3 ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3e8d899-38ef-4e5b-b798-782d97c6144e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from time import strftime\n",
    "from functools import partial\n",
    "import importlib\n",
    "\n",
    "import utilities as u\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.experiments.run import Run\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc08ea8e-787f-42ae-bdba-4f3d8ba9c044",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.215.0', '1.34.84')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__, boto3.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46afa5b6-7d8e-4d4e-9be7-bedaf774694b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assumed SageMaker role is arn:aws:iam::111918798052:role/DevelopmentRole\n",
      "S3_DATA_URI s3://sagemaker-us-east-1-111918798052/data/\n"
     ]
    }
   ],
   "source": [
    "boto_session = boto3.session.Session()\n",
    "sagemaker_session = sagemaker.session.Session(boto_session)\n",
    "omics = boto3.client(\"omics\")\n",
    "\n",
    "REGION_NAME = sagemaker_session.boto_region_name\n",
    "S3_BUCKET = sagemaker_session.default_bucket()\n",
    "\n",
    "EXPERIMENT_NAME = \"hyenaDNA-pretraining-v2\"\n",
    "\n",
    "SAGEMAKER_EXECUTION_ROLE = sagemaker.session.get_execution_role(sagemaker_session)\n",
    "print(f\"Assumed SageMaker role is {SAGEMAKER_EXECUTION_ROLE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd25e8-4256-49b5-97ef-b98ed7f0657e",
   "metadata": {},
   "source": [
    "### 1. Read the data from AWS HealthOmics\n",
    "\n",
    "To train the HyenaDNA model, we will directly utilize the data from the Healthomics Sequence Store. Let's retrieve the read set URI from the Healthomics Sequence Store. Make sure to replace your sequence store id in the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bebb2c-f925-4532-8c03-60195f26d657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seq_store_id = \"4308389581\" # replace with your sequence store id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4de9f99-49cd-425c-b724-b27ed95ad750",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('s3://111918798052-4308389-m7r4grkrg7nkpmf5swnjwf1iqsdieuse1b-s3alias/111918798052/sequenceStore/4308389581/',\n",
       " 'arn:aws:s3:us-east-1:559620149354:accesspoint/111918798052-4308389581',\n",
       " 'arn:aws:kms:us-east-1:559620149354:key/ef42c6a8-5692-4a6c-9a66-a2d1058a9a41')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_store_info = omics.get_sequence_store(id=seq_store_id)\n",
    "s3_uri = seq_store_info[\"s3Access\"][\"s3Uri\"]\n",
    "s3_arn = seq_store_info[\"s3Access\"][\"s3AccessPointArn\"]\n",
    "key_arn = seq_store_info[\"sseConfig\"][\"keyArn\"]\n",
    "s3_uri, s3_arn, key_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207ee04b-a852-4533-beff-544e38fe5952",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_DATA_URI = f\"{s3_uri}readSet/\"\n",
    "S3_DATA_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6120bb71-a43e-4e15-bc36-800ae4c320ed",
   "metadata": {},
   "source": [
    "We will be directly downloadin the For the training job to access the objects in the above S3 access point, `s3uri`, you must add a policy\n",
    "to this execution role (`SAGEMAKER_EXECUTION_ROLE`). The output of the following cell is the policy that\n",
    "you should attach to this role:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63c953ee-7029-4562-b008-76b8f7594e81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Version\": \"2012-10-17\",\n",
      "  \"Statement\": [\n",
      "    {\n",
      "      \"Sid\": \"S3DirectAccess\",\n",
      "      \"Effect\": \"Allow\",\n",
      "      \"Action\": [\n",
      "        \"s3:GetObject\",\n",
      "        \"s3:ListBucket\"\n",
      "      ],\n",
      "      \"Resource\": \"*\",\n",
      "      \"Condition\": {\n",
      "        \"StringEquals\": {\n",
      "          \"s3:DataAccessPointArn\": \"arn:aws:s3:us-east-1:559620149354:accesspoint/111918798052-4308389581\"\n",
      "        }\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"Sid\": \"DefaultSequenceStoreKMSDecrypt\",\n",
      "      \"Effect\": \"Allow\",\n",
      "      \"Action\": \"kms:Decrypt\",\n",
      "      \"Resource\": \"arn:aws:kms:us-east-1:559620149354:key/ef42c6a8-5692-4a6c-9a66-a2d1058a9a41\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps({\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        { \n",
    "            \"Sid\": \"S3DirectAccess\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [ \n",
    "                \"s3:GetObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ], \n",
    "            \"Resource\": \"*\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"s3:DataAccessPointArn\": s3_arn\n",
    "                } \n",
    "            }\n",
    "        },\n",
    "        { \n",
    "            \"Sid\": \"DefaultSequenceStoreKMSDecrypt\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": \"kms:Decrypt\",\n",
    "            \"Resource\": key_arn\n",
    "        }\n",
    "    ] \n",
    "}, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb148d5b-b950-4ee4-810e-75a56e97565e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://111918798052-4308389-m7r4grkrg7nkpmf5swnjwf1iqsdieuse1b-s3alias/111918798052/sequenceStore/4308389581/readSet/'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ab4d0b7-0eb9-4757-aa89-aec2ca107115",
   "metadata": {},
   "source": [
    "### 2. Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a68d1b-edb1-49ee-acae-076bdecff096",
   "metadata": {},
   "source": [
    "### 2.1 Define the training container "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7e3deae-0de2-499e-a60a-c76aaef138ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.2.0-gpu-py310-cu121-ubuntu20.04-sagemaker'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_image_uri = f\"763104351884.dkr.ecr.{REGION_NAME}.amazonaws.com/pytorch-training:2.2.0-gpu-py310-cu121-ubuntu20.04-sagemaker\"\n",
    "pytorch_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52a3e14-2cab-472a-96d9-808c39c4f8c8",
   "metadata": {},
   "source": [
    "#### 2.2 Define the training job parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cfe0c7f6-0a53-483a-80ae-793ad58d7ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 'LongSafari/hyenadna-small-32k-seqlen-hf'\n",
    "TRAINING_JOB_NAME = 'hyenaDNA-pretraining'\n",
    "\n",
    "# Additional training parameters\n",
    "hyperparameters = {\n",
    "    \"species\" : \"mouse\",\n",
    "    \"epochs\": 150,\n",
    "    \"model_checkpoint\": MODEL_ID,\n",
    "    \"max_length\": 32_000,\n",
    "    \"batch_size\": 4, \n",
    "    \"learning_rate\": 6e-4,\n",
    "    \"weight_decay\" : 0.1,\n",
    "    \"log_level\" : \"INFO\",\n",
    "    \"log_interval\" : 100\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3178423a-828b-4101-b540-5efe22b5e223",
   "metadata": {},
   "source": [
    "#### 2.3 Define Metrics to track\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4ad98da5-1d11-4eb4-8718-2e4cb4834872",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    {\"Name\": \"epoch\", \"Regex\": \"Epoch: ([0-9.]*)\"},\n",
    "    {\"Name\": \"step\", \"Regex\": \"Step: ([0-9.]*)\"},\n",
    "    {\"Name\": \"train_loss\", \"Regex\": \"Train Loss: ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"train_perplexity\", \"Regex\": \"Train Perplexity: ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"eval_loss\", \"Regex\": \"Eval Average Loss: ([0-9.e-]*)\"},\n",
    "    {\"Name\": \"eval_perplexity\", \"Regex\": \"Eval Perplexity: ([0-9.e-]*)\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c9636d-1707-42ee-aa77-3f63c095285c",
   "metadata": {},
   "source": [
    "#### 2.4 Define the tensorboard configurations to track the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d22067bf-c6b5-478d-b8ca-65a0bd487768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import TensorBoardOutputConfig\n",
    "\n",
    "LOG_DIR=\"/opt/ml/output/tensorboard\"\n",
    "\n",
    "output_path = os.path.join(\n",
    "    \"s3://\", S3_BUCKET, \"sagemaker-output\", \"training\", TRAINING_JOB_NAME\n",
    ")\n",
    "\n",
    "tensorboard_output_config = TensorBoardOutputConfig(\n",
    "    s3_output_path=os.path.join(output_path, 'tensorboard'),\n",
    "    container_local_output_path=LOG_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af18fb5e-e1cf-447b-84fa-eb80b347c9e1",
   "metadata": {},
   "source": [
    "#### 2.4 Define Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dcd8126d-605c-40c6-bce1-a68602f80e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyenaDNA_estimator = PyTorch(\n",
    "    base_job_name=TRAINING_JOB_NAME,\n",
    "    entry_point=\"train_hf_accelerate.py\",\n",
    "    source_dir=\"scripts/\",\n",
    "    instance_type=\"ml.g5.12xlarge\",\n",
    "    instance_count=1,\n",
    "    image_uri=pytorch_image_uri,\n",
    "    role=SAGEMAKER_EXECUTION_ROLE,\n",
    "    hyperparameters=hyperparameters,\n",
    "    metric_definitions=metric_definitions,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    tags=[{\"Key\": \"project\", \"Value\": \"genomics-model-pretraining\"}],\n",
    "    keep_alive_period_in_seconds=1800,\n",
    "    tensorboard_output_config=tensorboard_output_config,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea04af1c-82ce-44ca-bce7-76d9776c6d95",
   "metadata": {},
   "source": [
    "#### 2.5 Start Training with Distributed Data Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b5544d-cffb-4a1a-bbe5-5d260c31bf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: hyenaDNA-pretraining-2024-04-16-00-12-50-506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-04-16 00:12:50 Starting - Starting the training job...\n",
      "2024-04-16 00:13:08 Pending - Training job waiting for capacity...\n",
      "2024-04-16 00:13:34 Pending - Preparing the instances for training...\n",
      "2024-04-16 00:14:16 Downloading - Downloading input data...\n",
      "2024-04-16 00:14:31 Downloading - Downloading the training image...............\n",
      "2024-04-16 00:17:12 Training - Training image download completed. Training in progress.......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-04-16 00:18:08,487 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-04-16 00:18:08,546 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-16 00:18:08,556 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-04-16 00:18:08,557 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[34m2024-04-16 00:18:08,557 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-04-16 00:18:09,851 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.2.1)\u001b[0m\n",
      "\u001b[34mCollecting s3torchconnector (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading s3torchconnector-1.2.3.tar.gz (55 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.9/55.9 kB 6.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling build dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: started\u001b[0m\n",
      "\u001b[34mGetting requirements to build wheel: finished with status 'done'\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: started\u001b[0m\n",
      "\u001b[34mInstalling backend dependencies: finished with status 'done'\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.26.1 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.26.1-py3-none-any.whl.metadata (100 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.3/100.3 kB 9.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting biopython==1.73 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading biopython-1.73.tar.gz (15.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 15.7/15.7 MB 87.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting pyfaidx==0.8.1.1 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading pyfaidx-0.8.1.1-py3-none-any.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.28.0 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard==2.16.2 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.16.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1->-r requirements.txt (line 4)) (3.13.1)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.11.0 (from transformers==4.26.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1->-r requirements.txt (line 4)) (23.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1->-r requirements.txt (line 4)) (6.0.1)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers==4.26.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 7.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1->-r requirements.txt (line 4)) (2.31.0)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.26.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.26.1->-r requirements.txt (line 4)) (4.66.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from pyfaidx==0.8.1.1->-r requirements.txt (line 6)) (68.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.10/site-packages (from pyfaidx==0.8.1.1->-r requirements.txt (line 6)) (6.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.28.0->-r requirements.txt (line 7)) (5.9.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.28.0->-r requirements.txt (line 7)) (2.2.0)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.3.1 (from accelerate==0.28.0->-r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.10/site-packages (from tensorboard==2.16.2->-r requirements.txt (line 8)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: grpcio>=1.48.2 in /opt/conda/lib/python3.10/site-packages (from tensorboard==2.16.2->-r requirements.txt (line 8)) (1.62.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard==2.16.2->-r requirements.txt (line 8)) (3.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /opt/conda/lib/python3.10/site-packages (from tensorboard==2.16.2->-r requirements.txt (line 8)) (3.20.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>1.9 in /opt/conda/lib/python3.10/site-packages (from tensorboard==2.16.2->-r requirements.txt (line 8)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard==2.16.2->-r requirements.txt (line 8)) (0.7.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard==2.16.2->-r requirements.txt (line 8)) (3.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 2)) (2024.1)\u001b[0m\n",
      "\u001b[34mCollecting s3torchconnectorclient>=1.2.3 (from s3torchconnector->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading s3torchconnectorclient-1.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1->-r requirements.txt (line 4)) (2024.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers==4.26.1->-r requirements.txt (line 4)) (4.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.28.0->-r requirements.txt (line 7)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.28.0->-r requirements.txt (line 7)) (3.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.28.0->-r requirements.txt (line 7)) (3.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard==2.16.2->-r requirements.txt (line 8)) (2.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata->pyfaidx==0.8.1.1->-r requirements.txt (line 6)) (3.17.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.26.1->-r requirements.txt (line 4)) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.26.1->-r requirements.txt (line 4)) (3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.26.1->-r requirements.txt (line 4)) (1.26.18)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.26.1->-r requirements.txt (line 4)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.28.0->-r requirements.txt (line 7)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.3/6.3 MB 101.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pyfaidx-0.8.1.1-py3-none-any.whl (28 kB)\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.28.0-py3-none-any.whl (290 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 290.1/290.1 kB 40.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 388.9/388.9 kB 48.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 774.0/774.0 kB 79.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading s3torchconnectorclient-1.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.7/3.7 MB 101.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 76.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 108.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: biopython, s3torchconnector\u001b[0m\n",
      "\u001b[34mBuilding wheel for biopython (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for biopython (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for biopython: filename=biopython-1.73-cp310-cp310-linux_x86_64.whl size=2171090 sha256=adab56dac5446ee29ad64dd4e204364394e989a0f795e80b5c0e0adcc3d8b980\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/23/52/b9/1339217891a98d1331331b3d9159dae6f9116e40715a56a950\u001b[0m\n",
      "\u001b[34mBuilding wheel for s3torchconnector (pyproject.toml): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for s3torchconnector (pyproject.toml): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for s3torchconnector: filename=s3torchconnector-1.2.3-py3-none-any.whl size=61036 sha256=605ed731351da1e7c5e2014ee2f9a8bfc25dc03499ed3b8622f4f724d272f3e4\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/38/94/ff/8eb2fafd1ec4eedbaf3e155715960acdb9d2c1a1c96dfa062c\u001b[0m\n",
      "\u001b[34mSuccessfully built biopython s3torchconnector\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tokenizers, safetensors, s3torchconnectorclient, regex, biopython, pyfaidx, huggingface-hub, transformers, s3torchconnector, accelerate\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.22.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.22.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.22.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.28.0 biopython-1.73 huggingface-hub-0.22.2 pyfaidx-0.8.1.1 regex-2023.12.25 s3torchconnector-1.2.3 s3torchconnectorclient-1.2.3 safetensors-0.4.3 tokenizers-0.13.3 transformers-4.26.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-04-16 00:18:31,568 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-04-16 00:18:31,568 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-04-16 00:18:31,640 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-16 00:18:31,706 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-16 00:18:31,717 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[34m2024-04-16 00:18:31,772 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-04-16 00:18:31,783 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"data\": \"/opt/ml/input/data/data\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 4,\n",
      "        \"epochs\": 150,\n",
      "        \"learning_rate\": 0.0006,\n",
      "        \"log_interval\": 100,\n",
      "        \"log_level\": \"INFO\",\n",
      "        \"max_length\": 32000,\n",
      "        \"model_checkpoint\": \"LongSafari/hyenadna-small-32k-seqlen-hf\",\n",
      "        \"species\": \"mouse\",\n",
      "        \"weight_decay\": 0.1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"data\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": false,\n",
      "    \"job_name\": \"hyenaDNA-pretraining-2024-04-16-00-12-50-506\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-111918798052/hyenaDNA-pretraining-2024-04-16-00-12-50-506/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_hf_accelerate\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_hf_accelerate.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":4,\"epochs\":150,\"learning_rate\":0.0006,\"log_interval\":100,\"log_level\":\"INFO\",\"max_length\":32000,\"model_checkpoint\":\"LongSafari/hyenadna-small-32k-seqlen-hf\",\"species\":\"mouse\",\"weight_decay\":0.1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_hf_accelerate.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.12xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"data\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"data\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_hf_accelerate\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-111918798052/hyenaDNA-pretraining-2024-04-16-00-12-50-506/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.12xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"data\":\"/opt/ml/input/data/data\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":4,\"epochs\":150,\"learning_rate\":0.0006,\"log_interval\":100,\"log_level\":\"INFO\",\"max_length\":32000,\"model_checkpoint\":\"LongSafari/hyenadna-small-32k-seqlen-hf\",\"species\":\"mouse\",\"weight_decay\":0.1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"data\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":false,\"job_name\":\"hyenaDNA-pretraining-2024-04-16-00-12-50-506\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-111918798052/hyenaDNA-pretraining-2024-04-16-00-12-50-506/source/sourcedir.tar.gz\",\"module_name\":\"train_hf_accelerate\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_hf_accelerate.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"4\",\"--epochs\",\"150\",\"--learning_rate\",\"0.0006\",\"--log_interval\",\"100\",\"--log_level\",\"INFO\",\"--max_length\",\"32000\",\"--model_checkpoint\",\"LongSafari/hyenadna-small-32k-seqlen-hf\",\"--species\",\"mouse\",\"--weight_decay\",\"0.1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_DATA=/opt/ml/input/data/data\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=150\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0006\u001b[0m\n",
      "\u001b[34mSM_HP_LOG_INTERVAL=100\u001b[0m\n",
      "\u001b[34mSM_HP_LOG_LEVEL=INFO\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_LENGTH=32000\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_CHECKPOINT=LongSafari/hyenadna-small-32k-seqlen-hf\u001b[0m\n",
      "\u001b[34mSM_HP_SPECIES=mouse\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mtorchrun --nnodes 1 --nproc_per_node 4 train_hf_accelerate.py --batch_size 4 --epochs 150 --learning_rate 0.0006 --log_interval 100 --log_level INFO --max_length 32000 --model_checkpoint LongSafari/hyenadna-small-32k-seqlen-hf --species mouse --weight_decay 0.1\u001b[0m\n",
      "\u001b[34m[2024-04-16 00:18:32,896] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-04-16 00:18:32,896] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-04-16 00:18:32,896] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-04-16 00:18:32,896] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:18:42 - INFO - __main__ - Starting training.......\u001b[0m\n",
      "\u001b[34mStatrted the pre-processing process\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:18:43 - INFO - __main__ - Starting training.......\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:18:43 - INFO - __main__ - Starting training.......\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:18:43 - INFO - __main__ - Starting training.......\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/4019662486/chr5.fq.gz -> /opt/ml/input/data/data/4019662486/chr5.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/4019662486/chr5.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/5832638637/chr14.fq.gz -> /opt/ml/input/data/data/5832638637/chr14.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/5832638637/chr14.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/5628991692/chr19.fq.gz -> /opt/ml/input/data/data/5628991692/chr19.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/5628991692/chr19.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/4552296085/chr11.fq.gz -> /opt/ml/input/data/data/4552296085/chr11.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/4552296085/chr11.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/4399289471/chr1.fq.gz -> /opt/ml/input/data/data/4399289471/chr1.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/4399289471/chr1.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/2043193651/chr17.fq.gz -> /opt/ml/input/data/data/2043193651/chr17.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/2043193651/chr17.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/4291333584/chr10.fq.gz -> /opt/ml/input/data/data/4291333584/chr10.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/4291333584/chr10.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/7039739896/chr13.fq.gz -> /opt/ml/input/data/data/7039739896/chr13.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/7039739896/chr13.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/2677481661/chr8.fq.gz -> /opt/ml/input/data/data/2677481661/chr8.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/2677481661/chr8.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/5719009676/chr4.fq.gz -> /opt/ml/input/data/data/5719009676/chr4.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/5719009676/chr4.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/2063114856/chr16.fq.gz -> /opt/ml/input/data/data/2063114856/chr16.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/2063114856/chr16.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/6611795136/chr18.fq.gz -> /opt/ml/input/data/data/6611795136/chr18.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/6611795136/chr18.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/4815912818/chrX.fq.gz -> /opt/ml/input/data/data/4815912818/chrX.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/4815912818/chrX.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/8963750725/chr15.fq.gz -> /opt/ml/input/data/data/8963750725/chr15.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/8963750725/chr15.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/1040759226/chr12.fq.gz -> /opt/ml/input/data/data/1040759226/chr12.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/1040759226/chr12.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/4164737562/chr9.fq.gz -> /opt/ml/input/data/data/4164737562/chr9.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/4164737562/chr9.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/9274193512/chr6.fq.gz -> /opt/ml/input/data/data/9274193512/chr6.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/9274193512/chr6.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/8761700955/chr7.fq.gz -> /opt/ml/input/data/data/8761700955/chr7.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/8761700955/chr7.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/7048177625/chr3.fq.gz -> /opt/ml/input/data/data/7048177625/chr3.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/7048177625/chr3.fq.gz\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/6849506425/chr2.fq.gz -> /opt/ml/input/data/data/6849506425/chr2.fq\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/6849506425/chr2.fq.gz\u001b[0m\n",
      "\u001b[34mDone\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/4019662486/chr5.fq -> /opt/ml/input/data/data/4019662486/chr5.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/4019662486/chr5.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/5832638637/chr14.fq -> /opt/ml/input/data/data/5832638637/chr14.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/5832638637/chr14.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/5628991692/chr19.fq -> /opt/ml/input/data/data/5628991692/chr19.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/5628991692/chr19.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/4552296085/chr11.fq -> /opt/ml/input/data/data/4552296085/chr11.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/4552296085/chr11.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/4399289471/chr1.fq -> /opt/ml/input/data/data/4399289471/chr1.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/4399289471/chr1.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/2043193651/chr17.fq -> /opt/ml/input/data/data/2043193651/chr17.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/2043193651/chr17.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/4291333584/chr10.fq -> /opt/ml/input/data/data/4291333584/chr10.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/4291333584/chr10.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/7039739896/chr13.fq -> /opt/ml/input/data/data/7039739896/chr13.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/7039739896/chr13.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/2677481661/chr8.fq -> /opt/ml/input/data/data/2677481661/chr8.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/2677481661/chr8.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/5719009676/chr4.fq -> /opt/ml/input/data/data/5719009676/chr4.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/5719009676/chr4.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/2063114856/chr16.fq -> /opt/ml/input/data/data/2063114856/chr16.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/2063114856/chr16.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/6611795136/chr18.fq -> /opt/ml/input/data/data/6611795136/chr18.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/6611795136/chr18.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/4815912818/chrX.fq -> /opt/ml/input/data/data/4815912818/chrX.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/4815912818/chrX.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/8963750725/chr15.fq -> /opt/ml/input/data/data/8963750725/chr15.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/8963750725/chr15.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/1040759226/chr12.fq -> /opt/ml/input/data/data/1040759226/chr12.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/1040759226/chr12.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/4164737562/chr9.fq -> /opt/ml/input/data/data/4164737562/chr9.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/4164737562/chr9.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/9274193512/chr6.fq -> /opt/ml/input/data/data/9274193512/chr6.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/9274193512/chr6.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/8761700955/chr7.fq -> /opt/ml/input/data/data/8761700955/chr7.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/8761700955/chr7.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/7048177625/chr3.fq -> /opt/ml/input/data/data/7048177625/chr3.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/7048177625/chr3.fq\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/data/6849506425/chr2.fq -> /opt/ml/input/data/data/6849506425/chr2.fa\u001b[0m\n",
      "\u001b[34mDeleted /opt/ml/input/data/data/6849506425/chr2.fq\u001b[0m\n",
      "\u001b[34mDone\u001b[0m\n",
      "\u001b[34mCompleted the pre-processing of the data\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.4+cuda12.1\u001b[0m\n",
      "\u001b[34malgo-1:143:467 [0] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:145:470 [2] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:144:469 [1] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34malgo-1:146:468 [3] configure_nvls_option:293 NCCL WARN NET/OFI Could not find ncclGetVersion symbol\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:19:27 - INFO - __main__ - Accelerate State: Distributed environment: MULTI_GPU  Backend: nccl\u001b[0m\n",
      "\u001b[34mNum processes: 4\u001b[0m\n",
      "\u001b[34mProcess index: 0\u001b[0m\n",
      "\u001b[34mLocal process index: 0\u001b[0m\n",
      "\u001b[34mDevice: cuda:0\u001b[0m\n",
      "\u001b[34mMixed precision type: no\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:19:27 - INFO - __main__ - Is Main Process: True\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:19:27 - INFO - __main__ - Local Process Index: 0\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:19:27 - INFO - __main__ - Device: cuda:0\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:19:27 - INFO - __main__ - Number of Processes: 4\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mSpecies: mouse\u001b[0m\n",
      "\u001b[34mSplit: train\u001b[0m\n",
      "\u001b[34mChromosomes: ['2', '4', '6', '8', '14', '15', '16', '17', '18', '19', 'X']\u001b[0m\n",
      "\u001b[34mLoaded 11 FASTA files from /opt/ml/input/data/data/mouse: ['2', '4', '6', '8', '14', '15', '16', '17', '18', '19', 'X']\u001b[0m\n",
      "\u001b[34mSpecies weights: [('mouse', 1)]\u001b[0m\n",
      "\u001b[34mChromosome weights: {'mouse': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\u001b[0m\n",
      "\u001b[34mSpecies: mouse\u001b[0m\n",
      "\u001b[34mSplit: train\u001b[0m\n",
      "\u001b[34mChromosomes: ['2', '4', '6', '8', '14', '15', '16', '17', '18', '19', 'X']\u001b[0m\n",
      "\u001b[34mLoaded 11 FASTA files from /opt/ml/input/data/data/mouse: ['2', '4', '6', '8', '14', '15', '16', '17', '18', '19', 'X']\u001b[0m\n",
      "\u001b[34mSpecies weights: [('mouse', 1)]\u001b[0m\n",
      "\u001b[34mChromosome weights: {'mouse': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\u001b[0m\n",
      "\u001b[34mSpecies: mouse\u001b[0m\n",
      "\u001b[34mSplit: train\u001b[0m\n",
      "\u001b[34mChromosomes: ['2', '4', '6', '8', '14', '15', '16', '17', '18', '19', 'X']\u001b[0m\n",
      "\u001b[34mLoaded 11 FASTA files from /opt/ml/input/data/data/mouse: ['2', '4', '6', '8', '14', '15', '16', '17', '18', '19', 'X']\u001b[0m\n",
      "\u001b[34mSpecies weights: [('mouse', 1)]\u001b[0m\n",
      "\u001b[34mChromosome weights: {'mouse': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\u001b[0m\n",
      "\u001b[34mSpecies: mouse\u001b[0m\n",
      "\u001b[34mSplit: train\u001b[0m\n",
      "\u001b[34mChromosomes: ['2', '4', '6', '8', '14', '15', '16', '17', '18', '19', 'X']\u001b[0m\n",
      "\u001b[34mLoaded 11 FASTA files from /opt/ml/input/data/data/mouse: ['2', '4', '6', '8', '14', '15', '16', '17', '18', '19', 'X']\u001b[0m\n",
      "\u001b[34mSpecies weights: [('mouse', 1)]\u001b[0m\n",
      "\u001b[34mChromosome weights: {'mouse': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\u001b[0m\n",
      "\u001b[34mSpecies: mouse\u001b[0m\n",
      "\u001b[34mSplit: test\u001b[0m\n",
      "\u001b[34mChromosomes: ['5', '7', '9', '10', '11']\u001b[0m\n",
      "\u001b[34mLoaded 5 FASTA files from /opt/ml/input/data/data/mouse: ['5', '7', '9', '10', '11']\u001b[0m\n",
      "\u001b[34mSpecies weights: [('mouse', 1)]\u001b[0m\n",
      "\u001b[34mChromosome weights: {'mouse': [1, 1, 1, 1, 1]}\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mSpecies: mouse\u001b[0m\n",
      "\u001b[34mSplit: test\u001b[0m\n",
      "\u001b[34mChromosomes: ['5', '7', '9', '10', '11']\u001b[0m\n",
      "\u001b[34mLoaded 5 FASTA files from /opt/ml/input/data/data/mouse: ['5', '7', '9', '10', '11']\u001b[0m\n",
      "\u001b[34mSpecies weights: [('mouse', 1)]\u001b[0m\n",
      "\u001b[34mChromosome weights: {'mouse': [1, 1, 1, 1, 1]}\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mSpecies: mouse\u001b[0m\n",
      "\u001b[34mSplit: test\u001b[0m\n",
      "\u001b[34mChromosomes: ['5', '7', '9', '10', '11']\u001b[0m\n",
      "\u001b[34mLoaded 5 FASTA files from /opt/ml/input/data/data/mouse: ['5', '7', '9', '10', '11']\u001b[0m\n",
      "\u001b[34mSpecies weights: [('mouse', 1)]\u001b[0m\n",
      "\u001b[34mChromosome weights: {'mouse': [1, 1, 1, 1, 1]}\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mSpecies: mouse\u001b[0m\n",
      "\u001b[34mSplit: test\u001b[0m\n",
      "\u001b[34mChromosomes: ['5', '7', '9', '10', '11']\u001b[0m\n",
      "\u001b[34mLoaded 5 FASTA files from /opt/ml/input/data/data/mouse: ['5', '7', '9', '10', '11']\u001b[0m\n",
      "\u001b[34mSpecies weights: [('mouse', 1)]\u001b[0m\n",
      "\u001b[34mChromosome weights: {'mouse': [1, 1, 1, 1, 1]}\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34mExplicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:19:34 - INFO - __main__ - Train Epoch: 0 [0/1000 (0%)], Train Loss: 1.489712, Train Perplexity: 4.435820\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:20:32 - INFO - __main__ - \n",
      " Epoch 0. Eval Average Loss: 1.3521, Eval Perplexity: 3.865570\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:20:33 - INFO - __main__ - Train Epoch: 1 [0/1000 (0%)], Train Loss: 1.360629, Train Perplexity: 3.898643\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:21:30 - INFO - __main__ - \n",
      " Epoch 1. Eval Average Loss: 1.3276, Eval Perplexity: 3.772164\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:21:31 - INFO - __main__ - Train Epoch: 2 [0/1000 (0%)], Train Loss: 1.339716, Train Perplexity: 3.817961\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:22:28 - INFO - __main__ - \n",
      " Epoch 2. Eval Average Loss: 1.3330, Eval Perplexity: 3.792288\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:22:29 - INFO - __main__ - Train Epoch: 3 [0/1000 (0%)], Train Loss: 1.332461, Train Perplexity: 3.790360\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:23:26 - INFO - __main__ - \n",
      " Epoch 3. Eval Average Loss: 1.3370, Eval Perplexity: 3.807630\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:23:27 - INFO - __main__ - Train Epoch: 4 [0/1000 (0%)], Train Loss: 1.348118, Train Perplexity: 3.850174\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:24:24 - INFO - __main__ - \n",
      " Epoch 4. Eval Average Loss: 1.3329, Eval Perplexity: 3.792175\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:24:25 - INFO - __main__ - Train Epoch: 5 [0/1000 (0%)], Train Loss: 1.340511, Train Perplexity: 3.820995\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:25:22 - INFO - __main__ - \n",
      " Epoch 5. Eval Average Loss: 1.3368, Eval Perplexity: 3.807029\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:25:23 - INFO - __main__ - Train Epoch: 6 [0/1000 (0%)], Train Loss: 1.338611, Train Perplexity: 3.813741\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:26:20 - INFO - __main__ - \n",
      " Epoch 6. Eval Average Loss: 1.3365, Eval Perplexity: 3.805576\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:26:20 - INFO - __main__ - Train Epoch: 7 [0/1000 (0%)], Train Loss: 1.341302, Train Perplexity: 3.824018\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:27:17 - INFO - __main__ - \n",
      " Epoch 7. Eval Average Loss: 1.3235, Eval Perplexity: 3.756594\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:27:18 - INFO - __main__ - Train Epoch: 8 [0/1000 (0%)], Train Loss: 1.331602, Train Perplexity: 3.787104\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:28:15 - INFO - __main__ - \n",
      " Epoch 8. Eval Average Loss: 1.3275, Eval Perplexity: 3.771486\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:28:16 - INFO - __main__ - Train Epoch: 9 [0/1000 (0%)], Train Loss: 1.327300, Train Perplexity: 3.770850\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:29:13 - INFO - __main__ - \n",
      " Epoch 9. Eval Average Loss: 1.3205, Eval Perplexity: 3.745411\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:29:14 - INFO - __main__ - Train Epoch: 10 [0/1000 (0%)], Train Loss: 1.336599, Train Perplexity: 3.806076\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:30:11 - INFO - __main__ - \n",
      " Epoch 10. Eval Average Loss: 1.3329, Eval Perplexity: 3.792180\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:30:12 - INFO - __main__ - Train Epoch: 11 [0/1000 (0%)], Train Loss: 1.320526, Train Perplexity: 3.745391\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:31:09 - INFO - __main__ - \n",
      " Epoch 11. Eval Average Loss: 1.3248, Eval Perplexity: 3.761349\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:31:10 - INFO - __main__ - Train Epoch: 12 [0/1000 (0%)], Train Loss: 1.325242, Train Perplexity: 3.763097\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:32:07 - INFO - __main__ - \n",
      " Epoch 12. Eval Average Loss: 1.3123, Eval Perplexity: 3.714628\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:32:07 - INFO - __main__ - Train Epoch: 13 [0/1000 (0%)], Train Loss: 1.317984, Train Perplexity: 3.735882\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:33:05 - INFO - __main__ - \n",
      " Epoch 13. Eval Average Loss: 1.3229, Eval Perplexity: 3.754138\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:33:05 - INFO - __main__ - Train Epoch: 14 [0/1000 (0%)], Train Loss: 1.317069, Train Perplexity: 3.732465\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:34:03 - INFO - __main__ - \n",
      " Epoch 14. Eval Average Loss: 1.3212, Eval Perplexity: 3.747925\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:34:03 - INFO - __main__ - Train Epoch: 15 [0/1000 (0%)], Train Loss: 1.317705, Train Perplexity: 3.734839\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:35:01 - INFO - __main__ - \n",
      " Epoch 15. Eval Average Loss: 1.3123, Eval Perplexity: 3.714545\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:35:01 - INFO - __main__ - Train Epoch: 16 [0/1000 (0%)], Train Loss: 1.310200, Train Perplexity: 3.706915\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:35:58 - INFO - __main__ - \n",
      " Epoch 16. Eval Average Loss: 1.3084, Eval Perplexity: 3.700084\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:35:59 - INFO - __main__ - Train Epoch: 17 [0/1000 (0%)], Train Loss: 1.302890, Train Perplexity: 3.679917\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:36:56 - INFO - __main__ - \n",
      " Epoch 17. Eval Average Loss: 1.3075, Eval Perplexity: 3.697008\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:36:57 - INFO - __main__ - Train Epoch: 18 [0/1000 (0%)], Train Loss: 1.318418, Train Perplexity: 3.737502\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:37:54 - INFO - __main__ - \n",
      " Epoch 18. Eval Average Loss: 1.2831, Eval Perplexity: 3.607774\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:37:55 - INFO - __main__ - Train Epoch: 19 [0/1000 (0%)], Train Loss: 1.333914, Train Perplexity: 3.795870\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:38:52 - INFO - __main__ - \n",
      " Epoch 19. Eval Average Loss: 1.2553, Eval Perplexity: 3.508754\u001b[0m\n",
      "\u001b[34m[HyenaDNA Training]04/16/2024 00:38:53 - INFO - __main__ - Train Epoch: 20 [0/1000 (0%)], Train Loss: 1.305134, Train Perplexity: 3.688183\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "with Run(\n",
    "    experiment_name=EXPERIMENT_NAME,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ") as run:\n",
    "    hyenaDNA_estimator.fit(\n",
    "        {\n",
    "            \"data\": TrainingInput(\n",
    "                s3_data=S3_DATA_URI, input_mode=\"File\"\n",
    "            ),\n",
    "        },\n",
    "        wait=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5efe1b4b-f7d5-402c-93ed-0844a64fd581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hyenaDNA-pretraining-2024-04-15-23-08-08-173'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_job_name = hyenaDNA_estimator.latest_training_job.name\n",
    "training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa472a6e-7d94-4322-b9b7-90d9bfc9c92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59941ec0-9bb4-44f6-994d-eaa77b2adb10",
   "metadata": {},
   "source": [
    "### 5. Training Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6e66b6-92ce-4f8c-bc75-e884ee71b98a",
   "metadata": {},
   "source": [
    "* In our training process we had pushed the training resulsts to Tensorboard. You can see them using SageMaker tensorboad application. Execuate following cell to get link to the the tensorboard application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "e743bb7b-6791-45fe-b678-4c9bee380e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.interactive_apps.base_interactive_app:A presigned domain URL was generated. This is sensitive and should not be shared with others.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://studio-d-xgpxwyumgsdh.studio.us-east-1.sagemaker.aws/auth?token=eyJhbGciOiJIUzI1NiJ9.eyJmYXNDcmVkZW50aWFscyI6IkFZQURlSGFJOVVoZE5hTktFWXpoc0FVUjFNSUFYd0FCQUJWaGQzTXRZM0o1Y0hSdkxYQjFZbXhwWXkxclpYa0FSRUV3TjJWMWJYTlRjR0phZERWb2VIVkNhbEIyVDNWUGFuVmhSRVpGUmxWQlVXaG1hWFl2ZFVWaVNEbDJTR1pQVkZWTVFrbGxXa3RUYjJWWU9UaFpNQ3RwVVQwOUFBRUFCMkYzY3kxcmJYTUFTMkZ5YmpwaGQzTTZhMjF6T25WekxXVmhjM1F0TVRvNU9EQXpOVEk0TWpZeE1UVTZhMlY1TDJFNE9UZ3labVU0TFRFM056Y3ROR0kwWmkwNE9UVTVMV00yTldNeE9XSXhZak14TUFDNEFRSUJBSGdubFhwQlJKL2g4bXY4ek1wY292U0FSWk1INi8vbTBCY2lxRVYwbnNNREVRRVc3Y1VIdlpxUm5DSVhuL2ZGZDFGRkFBQUFmakI4QmdrcWhraUc5dzBCQndhZ2J6QnRBZ0VBTUdnR0NTcUdTSWIzRFFFSEFUQWVCZ2xnaGtnQlpRTUVBUzR3RVFRTUNZRlRlVmF6ZnVwSlBCczJBZ0VRZ0R2RTBmVzZGamNZMmw1SDUwT1ZJVVBDMnJEeEVROFNQV0pSa0pldko2OVJMSFVmaFFHTVdEbkZMdklPOFpVSFJGZ2VZUzB5ZC9uakk4bzlPQUlBQUFBQURBQUFFQUFBQUFBQUFBQUFBQUFBQUFDYlgxdVpwUzJzejBsMGZab3c4L1pyLy8vLy93QUFBQUVBQUFBQUFBQUFBQUFBQUFFQUFBUW44a2dPT1RHOEdRVnV6cGdNVHhGYVl1VEZ5cWpuWHFlTksrNEdzR1AyUjJ1Ym53QUNkenUrM0owOUFtTFJCWit2Q00raUpkS3ppU203dEJwZHZ2Skd0S2t2WkJHZDZEWTF5TEYwQ3kyaU1yK2FpTDRIeUpVWmovbWpZdkxBUmN5V3U0eE9BTEUwNDRXMHFiODZCVnNTLzBENFJYS013d0pPYSs3Wllzd1RZVTlDUnFIeFFoOFdGRXJFVGhlbjFEN2lZcHZGR25BV1BnY0VIVjNvbVI3ZUlaTHhGVXZJMFZtQkpXdU82ZE1Pci9kMTdjdWwzZ0R2R1FMYXltSUhpa2RsZDRidFdLOWVsdE81NXFiZzh1Ui9kdlZPNUZBeStEbkxsSXNxeFRaeXVCa1FaSXJoRDFLczN2QW9OZDhKTGJVc1lZUWRuTFFNZ0RyTmhjK3V6NWxsRUdNditxMTJuNGw3REUvRS9IUkIwSEVsT1FPeVZsNzN5RHlYYnRQa3FRYjZ1bFRISnR3TUJwQWhBMjlrcDExbG96V2w5NWd2cjFoZEdxZUhsWHNGQTMxYkZlRHZmVmp6aGNOTFFlQnF2RWE3dU9leHIzRkIwaUk3ZG4zSWxRTFVBejB4OUMwOTBFMmJkWUh5bkxJV1JiUDdrNm4wbU9lZWpDUUtSWkh1d0pNWEFNbEQ4UHIwcDQ0VWtZYXFwekxhbjhBbkZzbUZpL2tXeHBIMkE5REJGdFc4c2RtYVc0WkxQWDV3ampBM2FDVzJMamhrTVhkTDlZNEdrYW9TQnplM0EzZlZSMVVmUUEveW5WVTVRWXNpMXJRb1JUZHhRaWZlTUk1V1gzeHI4eENXR2RjT3hiVkFvRGNGR1RldCtLUnNVWGNpU3lxeTRZcHZ4V1JsN25IdDdodnJxeEt5Wi9acC9rUzhnZFpabjVTR1FKdFpzWElCeHgwUzQ1ZEo0dkllWkc2THo4OHFFR0NCWUVVTTBuV2svZENnR090b0VpNTI0M21wM2dMK2ZkRUtrOUc2UUd0SXN5MC9YdW5aSklLbWZkSm51MVN2K09PcG94VDdtQS92K0d4SEhTUUozWnJSTlE1dVJCaHpUQWg1alZxZjBwSDFkb0hhc0JjYkNZaCs3dE90OGNZZmxScWsvR3VJcUErbkIvQW1xdlUzTGcwZ05wQUF4QmFmQUZWWUw1SEllMk5DaVFwSG5sSW12bURUMHpFTUsxekZKVDhsQUdhOXVaMVcrM1VhQmI2RjlmU3NYck5KLzF0T1RqZGRjU0Nldy9VV2JnS2dwYmFmMnprRm9kb3RISHA4K0JXdTdpM2lkemg3T3Eyd0tDVVh3NGpZeUNGeDY0U0h4b1dZY1dSeHFRS2RZRUFxUFNVd0YwcFlyQ1I4YXFaWDR6bXpEMFA5Sk9ScWxINXhzaE9QYWljbVFVMmczOXdpa3JxWFhNdElsY2l0WGtSb0g5ejNxcWNTZHdReDZNZG8rYTExS3krQXVzdzhZeGZ5d2YvbkhJSEVIWW0ybEk1dlA4WTNzS1gzN3U0SGJJbjRKekVNM3Bxbk1LeGZyTFhjNi9SM2JuVEJ3R1BQNHRybE5MR3Nodk5jRzlZV1hIRHdha3R4MHJMa0xjTHBxLzA4QnZDbmg0Y04vRFFXWUhpWmV3MnJaWTZRaElaVlo3N3JTVnBrUlhUSG9PTGdWUW9McU13M3V1Mjl4N0ZXRlB3NVM3N2xaNExSdVFZVGFVem13dnBoNnZEb3dzaC93Sk01cEpvM0VDendLcStZQjVudUtQOU4zYkM2QU9YdWJLSmZIeEdrZkdIWjhGc25ReEpRN1B5aG5hUXNpbCtndmR2NSsvLzRZNXQ1S1o4WUdwenVLaElTMEVYODluMVM0NFM1cEtkY2YxWUFGQm15dVJnUUY1TUFaekJsQWpCaEN4dkJjemM2a2J3S1RkdEEyNFZjdTdmOHdSZ0w4Sk9VdTlEaUJiZDU0MXpodlRMWG81NmhPaDlFdWNScTRiMENNUUQxdmxpRDNoN2pOd2ZGVGJRSlF2aEc0L2lrWGlQS2M0VGxnZERQV2J1Q1NsK0kvOUh6c25ISjJnb0Q2ejVwRjcwPSIsImNpcGhlclRleHQiOiJBUUlCQUhnbmxYcEJSSi9oOG12OHpNcGNvdlNBUlpNSDYvL20wQmNpcUVWMG5zTURFUUVyNlhkclVFb2x5RTVzWDRDbkVhcHlBQUFBb2pDQm53WUpLb1pJaHZjTkFRY0dvSUdSTUlHT0FnRUFNSUdJQmdrcWhraUc5dzBCQndFd0hnWUpZSVpJQVdVREJBRXVNQkVFREs5ejV4b1JDa2kxdGZ5TjNRSUJFSUJiRlMrdUhMZndtbmN6RVlTdUI3WDc1Sk5IanNsMEdTYUJyMnBybThIS21uelhSY1ZCUkpQdWxNbnFxY0xwS0hEMEkxcy81K2V4TE1IV0VyYzJHMmoybVdmSmdYQ3hrKy9KWHk0SndWSmlQUU9oNk96bHNlK0xsN0xET3c9PSIsImlkZW50aXR5SWQiOiJzaGFtaWthIiwidXNlclByb2ZpbGVOYW1lIjoic2hhbWlrYSIsImxhbmRpbmdVcmlTY2hlbWUiOiJzdHVkaW8iLCJsYW5kaW5nVXJpRGVlcExpbmsiOiIiLCJzdWIiOiJkLXhncHh3eXVtZ3NkaCIsImV4cCI6MTcxMjg1MzUyMCwiaWF0IjoxNzEyODEwMzIwfQ.O_gqU_jziurfXLGz1ZveCVkFcBLBAszCqyXDLCvBL1A&redirect=TensorBoard&state=L3RlbnNvcmJvYXJkL2RlZmF1bHQvZGF0YS9wbHVnaW4vc2FnZW1ha2VyX2RhdGFfbWFuYWdlci9hZGRfZm9sZGVyX29yX2pvYj9SZWRpcmVjdD1UcnVlJk5hbWU9aHllbmFETkEtcHJldHJhaW5pbmctMjAyNC0wNC0xMS0wMS0xMS01MS0zODg='"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.interactive_apps.tensorboard import TensorBoardApp\n",
    "\n",
    "user_profile = \"shamika\"\n",
    "\n",
    "with open(\"/opt/ml/metadata/resource-metadata.json\", \"r\") as f:\n",
    "    app_metadata = json.loads(f.read())\n",
    "    sm_user_profile_name = app_metadata[\"SpaceName\"]\n",
    "    sm_domain_id = app_metadata[\"DomainId\"]\n",
    "\n",
    "tb_app = TensorBoardApp(REGION_NAME)\n",
    "tb_app.get_app_url(\n",
    "    training_job_name=training_job_name,\n",
    "    create_presigned_domain_url=True,           \n",
    "    domain_id=sm_domain_id,                 \n",
    "    user_profile_name=user_profile, \n",
    "    open_in_default_web_browser=False,\n",
    "    optional_create_presigned_url_kwargs={} \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c23d90b-f961-4f82-aa0c-46f7a6124c3a",
   "metadata": {},
   "source": [
    "### 6. Deploy trained model to an realtime endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "93e65ac9-4ebf-4866-8578-8d8706fe1036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2024-04-06 17:23:11 Starting - Starting the training job\n",
      "2024-04-06 17:23:11 Pending - Preparing the instances for training\n",
      "2024-04-06 17:23:11 Downloading - Downloading the training image\n",
      "2024-04-06 17:23:11 Training - Training image download completed. Training in progress.\n",
      "2024-04-06 17:23:11 Uploading - Uploading generated training model\n",
      "2024-04-06 17:23:11 Completed - Instances not retained as a result of warmpool resource limits being exceeded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-111918798052/hyenaDNA-pretraining-2024-04-06-06-23-26-412/output/model.tar.gz'"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "#training_job_name = \"hyenaDNA-pretraining-2024-04-06-06-23-26-412\"\n",
    "attached_estimator = Estimator.attach(training_job_name)\n",
    "\n",
    "model_data = attached_estimator.model_data\n",
    "model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "7f72c1f3-f658-4009-8fdd-c7e4d8bf548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the model to create a real-time endpoint\n",
    "endpoint_name = 'hyenaDNA-mouse-pretrained-ep'  \n",
    "pytorch_deployment_uri = f\"763104351884.dkr.ecr.{REGION}.amazonaws.com/pytorch-inference:2.2.0-gpu-py310-cu118-ubuntu20.04-sagemaker\"\n",
    "\n",
    "hyenaDNAModel = PyTorchModel(\n",
    "    model_data=model_data,\n",
    "    role=SAGEMAKER_EXECUTION_ROLE,\n",
    "    image_uri=pytorch_deployment_uri,\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=\"scripts/\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    name=endpoint_name,\n",
    "    env = {\n",
    "        'MMS_MAX_REQUEST_SIZE': '2000000000',\n",
    "        'MMS_MAX_RESPONSE_SIZE': '2000000000',\n",
    "        'MMS_DEFAULT_RESPONSE_TIMEOUT': '900',\n",
    "        'TS_MAX_RESPONSE_SIZE':'2000000000',\n",
    "        'TS_MAX_REQUEST_SIZE':'2000000000',\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "4808b94a-3079-40c9-bbe1-3fa596250803",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Repacking model artifact (s3://sagemaker-us-east-1-111918798052/hyenaDNA-pretraining-2024-04-06-06-23-26-412/output/model.tar.gz), script artifact (scripts/), and dependencies ([]) into single tar.gz file located at s3://sagemaker-us-east-1-111918798052/hyenaDNA-mouse-pretrained-ep-v8/model.tar.gz. This may take some time depending on model size...\n",
      "INFO:sagemaker:Creating model with name: hyenaDNA-mouse-pretrained-ep-v8\n",
      "INFO:sagemaker:Creating endpoint-config with name hyenaDNA-mouse-pretrained-real-ep-v8\n",
      "INFO:sagemaker:Creating endpoint with name hyenaDNA-mouse-pretrained-real-ep-v8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "real_time_endpoint_name = \"hyenaDNA-mouse-pretrained-real-ep-v8\"\n",
    "env = {\n",
    "    'SAGEMAKER_MODEL_SERVER_TIMEOUT':'7200', \n",
    "    'TS_MAX_RESPONSE_SIZE':'2000000000',\n",
    "    'TS_MAX_REQUEST_SIZE':'2000000000',\n",
    "    'MMS_MAX_RESPONSE_SIZE':'2000000000',\n",
    "    'MMS_MAX_REQUEST_SIZE':'2000000000'\n",
    "}\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "realtime_predictor = hyenaDNAModel.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.8xlarge\",\n",
    "    endpoint_name=real_time_endpoint_name,\n",
    "    env=env,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477dc76d-980c-4a21-8be7-0feba1ecf108",
   "metadata": {},
   "source": [
    "### 7. Test the realtime endpoint\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdd3233-f291-46e6-8818-138178857abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = [sample_genome_data[0]\n",
    "realtime_predictor.serializer = JSONSerializer()\n",
    "realtime_predictor.deserializer = JSONDeserializer()\n",
    "realtime_predictor.predict(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b53aedd-ce70-4e15-8ccd-7c0a08fecc24",
   "metadata": {},
   "source": [
    "#### 7. Cleanup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c9045b-1e15-41c9-85dc-775233388ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "realtime_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
